{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Introduction + Problem Definition\n",
        "In this notebook, 2 different Convolutional Neural Networks (CNNs) will be implemented and compared on a given datase. This dataset is the mnist dataset, as it is a standard dataset for training and testing a new model or comparing certain models.\n",
        "\n",
        "The 2 models that I will be comparing are the AlexNet and the ResNet. The reason why I am comparing these models will be explained before they get implemented."
      ],
      "metadata": {
        "id": "MRFVxOEPdPty"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTWRkNiZQWxw",
        "outputId": "a1f59e0d-e978-4fd7-b5ee-65cbc0ad8185"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "path = \"/content/gdrive/My Drive/DW_data/\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preparation\n",
        "In this section of the notebook, the labels will be converted into an array of integers so that it will be 10-ary classification of the data."
      ],
      "metadata": {
        "id": "uZv7sKCQffBN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0szOBrJAPwH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "885b7713-adc4-4c83-d3f7-dd561d6f413c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 2s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "#Create a binary classification matrix for the neural network.\n",
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "num_classes = 10\n",
        "def categorize(arr):\n",
        "  res = []\n",
        "  for i in range(len(arr)):\n",
        "    res.append([0 if arr[i] != j else 1 for j in range(num_classes)])\n",
        "    \n",
        "  return np.array(res)\n",
        "\n",
        "y_train = categorize(y_train)\n",
        "y_test = categorize(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Build the Models.\n",
        "In the following 2 code blocks, the AlexNet and the ResNet Models will be built."
      ],
      "metadata": {
        "id": "iyLbaWyPDEgX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjRoWpqBhTrV"
      },
      "source": [
        "The AlexNet model has been choosen because of the benefits that come from the number of layers (depth) of the network itself as it will be able to extract a lot of features from the source images (inputs). Although this comes with the burden of using a lot of computational resources, the use of Google Colab's GPU will reduce this restraint by some margin."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPt1ITfsBm5V"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Activation, BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, AveragePooling2D\n",
        "from keras.regularizers import l2\n",
        "\n",
        "#Initialize the AlexNet model.\n",
        "alexnet = Sequential()\n",
        "\n",
        "# Layer 1\n",
        "alexnet.add(Conv2D(96, (11, 11), input_shape=(28, 28, 1), padding='same', kernel_regularizer=l2(0.0)))\n",
        "alexnet.add(BatchNormalization())\n",
        "alexnet.add(Activation('relu'))\n",
        "alexnet.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Layer 2\n",
        "alexnet.add(Conv2D(256, (5, 5), padding='same'))\n",
        "alexnet.add(BatchNormalization())\n",
        "alexnet.add(Activation('relu'))\n",
        "alexnet.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Layer 3\n",
        "alexnet.add(ZeroPadding2D((1, 1)))\n",
        "alexnet.add(Conv2D(512, (3, 3), padding='same'))\n",
        "alexnet.add(BatchNormalization())\n",
        "alexnet.add(Activation('relu'))\n",
        "alexnet.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Layer 4\n",
        "alexnet.add(ZeroPadding2D((1, 1)))\n",
        "alexnet.add(Conv2D(1024, (3, 3), padding='same'))\n",
        "alexnet.add(BatchNormalization())\n",
        "alexnet.add(Activation('relu'))\n",
        "\n",
        "# Layer 5\n",
        "alexnet.add(ZeroPadding2D((1, 1)))\n",
        "alexnet.add(Conv2D(1024, (3, 3), padding='same'))\n",
        "alexnet.add(BatchNormalization())\n",
        "alexnet.add(Activation('relu'))\n",
        "alexnet.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Layer 6\n",
        "alexnet.add(Flatten())\n",
        "alexnet.add(Dense(3072))\n",
        "alexnet.add(BatchNormalization())\n",
        "alexnet.add(Activation('relu'))\n",
        "alexnet.add(Dropout(0.5))\n",
        "\n",
        "# Layer 7\n",
        "alexnet.add(Dense(4096))\n",
        "alexnet.add(BatchNormalization())\n",
        "alexnet.add(Activation('relu'))\n",
        "alexnet.add(Dropout(0.5))\n",
        "\n",
        "# Layer 8\n",
        "alexnet.add(Dense(num_classes))\n",
        "alexnet.add(BatchNormalization())\n",
        "alexnet.add(Activation('softmax'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6fGShtnh34V"
      },
      "source": [
        "ResNet's (Residual Network's) main benefit is that it can drop redundant layers that it does not need. This would allow for the model to be \"simplified\" in terms of it's layers and would allow for the error to be more easily reduced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5EvSaXnq1bw"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "def identity_block(x, filter):\n",
        "    # copy tensor to variable called x_skip\n",
        "    x_skip = x\n",
        "    # Layer 1\n",
        "    x = tf.keras.layers.Conv2D(filter, (3,3), padding = 'same')(x)\n",
        "    x = tf.keras.layers.BatchNormalization(axis=3)(x)\n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "    # Layer 2\n",
        "    x = tf.keras.layers.Conv2D(filter, (3,3), padding = 'same')(x)\n",
        "    x = tf.keras.layers.BatchNormalization(axis=3)(x)\n",
        "    # Add Residue\n",
        "    x = tf.keras.layers.Add()([x, x_skip])     \n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "def convolutional_block(x, filter):\n",
        "    # copy tensor to variable called x_skip\n",
        "    x_skip = x\n",
        "    # Layer 1\n",
        "    x = tf.keras.layers.Conv2D(filter, (3,3), padding = 'same', strides = (2,2))(x)\n",
        "    x = tf.keras.layers.BatchNormalization(axis=3)(x)\n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "    # Layer 2\n",
        "    x = tf.keras.layers.Conv2D(filter, (3,3), padding = 'same')(x)\n",
        "    x = tf.keras.layers.BatchNormalization(axis=3)(x)\n",
        "    # Processing Residue with conv(1,1)\n",
        "    x_skip = tf.keras.layers.Conv2D(filter, (1,1), strides = (2,2))(x_skip)\n",
        "    # Add Residue\n",
        "    x = tf.keras.layers.Add()([x, x_skip])     \n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "def ResNet34(shape = (28, 28, 1), classes = 10):\n",
        "    # Step 1 (Setup Input Layer)\n",
        "    x_input = tf.keras.layers.Input(shape)\n",
        "    x = tf.keras.layers.ZeroPadding2D((3, 3))(x_input)\n",
        "    # Step 2 (Initial Conv layer along with maxPool)\n",
        "    x = tf.keras.layers.Conv2D(64, kernel_size=7, strides=2, padding='same')(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "    x = tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding='same')(x)\n",
        "    # Define size of sub-blocks and initial filter size\n",
        "    block_layers = [3, 4, 6, 3]\n",
        "    filter_size = 64\n",
        "    # Step 3 Add the Resnet Blocks\n",
        "    for i in range(4):\n",
        "        if i == 0:\n",
        "            # For sub-block 1 Residual/Convolutional block not needed\n",
        "            for j in range(block_layers[i]):\n",
        "                x = identity_block(x, filter_size)\n",
        "        else:\n",
        "            # One Residual/Convolutional Block followed by Identity blocks\n",
        "            # The filter size will go on increasing by a factor of 2\n",
        "            filter_size = filter_size*2\n",
        "            x = convolutional_block(x, filter_size)\n",
        "            for j in range(block_layers[i] - 1):\n",
        "                x = identity_block(x, filter_size)\n",
        "    # Step 4 End Dense Network\n",
        "    x = tf.keras.layers.AveragePooling2D((2,2), padding = 'same')(x)\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "    x = tf.keras.layers.Dense(512, activation = 'relu')(x)\n",
        "    x = tf.keras.layers.Dense(classes, activation = 'softmax')(x)\n",
        "    model = tf.keras.models.Model(inputs = x_input, outputs = x, name = \"ResNet34\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Run and Compare Models"
      ],
      "metadata": {
        "id": "Nv0CdBhhCwXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alexnet.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(learning_rate=0.01),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "alexnet.fit(x_train, y_train,\n",
        "          batch_size=128,\n",
        "          epochs=5,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "\n",
        "score_alexnet = alexnet.evaluate(x_test, y_test, verbose=0)\n",
        "print('AlexNet Test loss:', score_alexnet[0])\n",
        "print('AlexNet Test accuracy:', score_alexnet[1])"
      ],
      "metadata": {
        "id": "hxjshwbrCzzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBRn1_A-rcj1",
        "outputId": "d697893b-6ce6-4a7f-9a09-7ac2b60587d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "469/469 [==============================] - 39s 76ms/step - loss: 0.3412 - accuracy: 0.8999 - val_loss: 0.1244 - val_accuracy: 0.9613\n",
            "Epoch 2/5\n",
            "469/469 [==============================] - 33s 71ms/step - loss: 0.0800 - accuracy: 0.9760 - val_loss: 0.0892 - val_accuracy: 0.9710\n",
            "Epoch 3/5\n",
            "469/469 [==============================] - 34s 72ms/step - loss: 0.0337 - accuracy: 0.9906 - val_loss: 0.0785 - val_accuracy: 0.9735\n",
            "Epoch 4/5\n",
            "469/469 [==============================] - 34s 72ms/step - loss: 0.0129 - accuracy: 0.9977 - val_loss: 0.0734 - val_accuracy: 0.9754\n",
            "Epoch 5/5\n",
            "469/469 [==============================] - 34s 72ms/step - loss: 0.0056 - accuracy: 0.9995 - val_loss: 0.0713 - val_accuracy: 0.9781\n",
            "Test loss: 0.07131729274988174\n",
            "Test accuracy: 0.9781000018119812\n"
          ]
        }
      ],
      "source": [
        "resnet = ResNet34()\n",
        "resnet.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(learning_rate=0.01),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "resnet.fit(x_train, y_train,\n",
        "          batch_size=128,\n",
        "          epochs=5,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "\n",
        "score_resnet = resnet.evaluate(x_test, y_test, verbose=0)\n",
        "print('ResNet Test loss:', score_resnet[0])\n",
        "print('ResNet Test accuracy:', score_resnet[1])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}